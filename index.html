<html>
<head>
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    <title>Platypus</title>
    <link href="./Platypus_files/style.css" rel="stylesheet">
</head>
<script type="text/javascript" src="./Platypus_files/jquery.js"></script>
<script type="text/javascript" src="./Platypus_files/jquery.mlens-1.0.min.js"></script>
<style>
    .gradio-center {
        display: flex;
        justify-content: center; /* Center horizontally */
        align-items: center; /* Center vertically */
        height: 100vh; /* Take up the full viewport height */
    }
</style>
</head>

<body>
<div class="content">
  <h1><strong>Platypus: Quick, Cheap, and Powerful Refinement of LLMs</strong></h1>
  
  <p id="authors"><span> <a href="https://www.arielnlee.com/">Ariel N. Lee<small>*</small></a> <a href="https://www.colejhunter.com/">Cole J. Hunter<small>*</small></a><a href="https://natanielruiz.github.io/"></a></span><a href="https://natanielruiz.github.io/">Nataniel Ruiz<small>†</small></a><br><br>
<small><small>* Equal Contribution. </small></small><br><small><small> † NR is currently at Google and his contributions were done as work at BU prior to his tenure at the company.</small></small><br>
    <br>
  <span style="font-size: 24px">Boston University
  </span></p>
  <br>
  
  <div style="width: 350px; height: 350px; border-radius: 50%; overflow: hidden; margin: 0 auto;">
    <img src="./Platypus_files/Best_Platty.jpeg" style="width: 100%; height: 100%; object-fit: cover;">
  </div>
  
  <br>
  <font size="+2">
    <p style="text-align: center;">
      <a href="https://arxiv.org/abs/2308.07317" target="_blank">[Paper]</a> &nbsp;&nbsp;&nbsp;&nbsp;
      <a href="https://huggingface.co/garage-bAInd" target="_blank">[Models]</a> &nbsp;&nbsp;&nbsp;&nbsp;
      <a href="https://huggingface.co/datasets/garage-bAInd/Open-Platypus" target="_blank">[Dataset]</a> &nbsp;&nbsp;&nbsp;&nbsp;
      <a href="https://github.com/arielnlee/Platypus" target="_blank">[Code]</a> &nbsp;&nbsp;&nbsp;&nbsp;
    </p>
  </font>
</div>
</body>

<div class="content">
  <h2 style="text-align:center;">Abstract</h2>
<p>We present <strong>Platypus</strong> a family of fine-tuned and merged Large Language Models (LLMs) that achieves the strongest performance and currently stands at first place in HuggingFace's <a href="https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard" target="_blank">Open LLM Leaderboard</a> as of the release date of this work. In this work we describe (1) our curated dataset <strong>Open-Platypus</strong>, that is a subset of other open datasets and which <em>we release to the public</em> (2) our process of fine-tuning and merging LoRA modules in order to conserve the strong prior of pretrained LLMs, while bringing specific domain knowledge to the surface (3) our efforts in checking for test data leaks and contamination in the training data, which can inform future research. Specifically, the Platypus family achieves strong performance in quantitative LLM metrics across model sizes, topping the global Open LLM leaderboard while using just a fraction of the fine-tuning data and overall compute that are required for other state-of-the-art fine-tuned LLMs. In particular, a 13B Platypus model can be trained on <em>a single</em> A100 GPU using 25k questions in 5 hours. This is a testament of the quality of our Open-Platypus dataset, and opens opportunities for more improvements in the field.</p>
</div>


<div class="gradio-center">
    <!-- Embedding the Gradio space -->
    <script
        type="module"
        src="https://gradio.s3-us-west-2.amazonaws.com/3.39.0/gradio.js"
    ></script>
    <gradio-app src="https://open-orca-openorca-platypus2-13b.hf.space"></gradio-app>
</div>


<div class="content">
  <h2>Background</h2>
  <p>Our research focuses on optimizing LLMs using PEFT and LoRA with our curated dataset, Open-Platypus. This is set against the backdrop of rapid advancements in LLMs, from the introduction of massive models like GPT-3 to task-specific ones like Galactica. While models like OpenAI's GPT-3.5 and GPT-4 have set high standards, open-source initiatives like BLOOM and Meta's LLaMa models have recently emerged as competitors. The challenge lies in fine-tuning these models efficiently. Our approach aims to harness the benefits of dataset distillation and instruction tuning, ensuring enhanced performance while emphasizing domain-specific knowledge.</p>
  </div>
  
<div class="content">
  <h2>Contributions</h2>
  <p><p>Our results reveal that domain-specific datasets enhance performance in selected tasks. When combined with merging, this significantly cuts down training duration. Key contributions include:</p>
<ul>
    <li><strong>Open-Platypus:</strong> A curated dataset derived from 11 open-source datasets, primarily focusing on enhancing LLMs' STEM and logic proficiency. Predominantly composed of human-crafted questions, with only about 10% generated by an LLM, Open-Platypus enables robust performance with minimal fine-tuning time and cost.</li><br>
    <li><strong>Dataset optimization:</strong> We detail our similarity exclusion approach, which helps in downsizing our dataset and minimizing data redundancy.</li><br>
    <li><strong>Addressing contamination:</strong> An in-depth exploration of the contamination issue in open LLM training sets, highlighting our data filtering process to circumvent this challenge.</li><br>
    <li><strong>Fine-tuning and merging:</strong> An overview of our selection, merging, and fine-tuning processes for LoRA modules, drawing inspiration from existing methodologies.</li>
</ul>
</p>
</div>

<div class="content">
  <h2>Open-Platypus</h2>
  <p>We release <a href="https://huggingface.co/datasets/garage-bAInd/Open-Platypus" target="_blank">Open-Platypus</a> to the public via Hugging Face. </p>
<style>
    table {
        width: 100%;
        border-collapse: collapse;
    }
    table, th, td {
        border: 1px solid #e0e0e0;
    }
    th, td {
        padding: 10px 15px;
    }
    thead th {
        background-color: #f7f7f7;
        border-bottom: 2px solid #e0e0e0;
    }
    tbody tr:hover {
        background-color: #f7f7f7;
    }
    caption {
        font-weight: bold;
        padding: 10px;
        text-align: center;
    }
    a {
        color: #224b8d;
        text-decoration: none;
    }
    a:hover {
        text-decoration: underline;
    }
    td a {
        font-weight: 500;
    }
</style>	
<table border="1" style="width:100%; text-align:left;">
    <caption style="text-align:center; padding-bottom:10px;">
        Table 1: Datasets, Licenses, and Number of Leaked Questions. *The datasets marked with asterisks were not added to Open-Platypus but we include them because we ran contamination checks when considering which models to merge.
    </caption>
    <thead>
        <tr>
            <th>Dataset Name</th>
            <th>License Type</th>
            <th># Leaked Questions</th>
        </tr>
    </thead>
    <tbody>
        <!-- Each row represents a dataset entry -->
        <tr>
            <td><a href="https://github.com/openai/prm800k">PRM800K: A Process Supervision Dataset</a></td>
            <td>MIT</td>
            <td>77</td>
        </tr>
        <tr>
            <td><a href="https://github.com/hendrycks/math">Measuring Mathematical Problem Solving With the MATH Dataset</td>
            <td>MIT</td>
            <td>77</td>
        </tr>
        <tr>
            <td><a href="https://github.com/lupantech/ScienceQA">ScienceQA: Science Question Answering</a></td>
            <td><a href="https://creativecommons.org/licenses/by-nc-sa/4.0/">Creative Commons Attribution-NonCommercial-ShareAlike 4.0</a></td>
            <td>0</td>
        </tr>
        <tr>
            <td><a href="https://github.com/mandyyyyii/scibench">SciBench: Evaluating College-Level Scientific Problem-Solving Abilities of Large Language Models</a></td>
            <td>MIT</td>
            <td>0</td>
        </tr>
        <tr>
            <td><a href="https://whyu.me/reclor/">ReClor: A Reading Comprehension Dataset Requiring Logical Reasoning</a></td>
            <td>Non-commercial</td>
            <td>0</td>
        </tr>
        <tr>
            <td><a href="https://arxiv.org/abs/1707.06209/">*SciQ: Crowdsourcing Multiple Choice Science Questions</td>
            <td><a href="https://creativecommons.org/licenses/by-nc/3.0/">Creative Commons Attribution-NonCommercial 3.0</a></td>
            <td>71</td>
        </tr>
        <tr>
            <td><a href="https://huggingface.co/datasets/wenhu/TheoremQA">TheoremQA: A Theorem-driven Question Answering Dataset</a></td>
            <td>MIT</td>
            <td>0</td>
        </tr>
        <tr>
            <td><a href="https://huggingface.co/datasets/nuprl/leetcode-solutions-python-testgen-gpt4/viewer/nuprl--leetcode-solutions-python-testgen-gpt4/train?p=1">leetcode-solutions-python-testgen-gpt4</a></td>
            <td>None listed</td>
            <td>0</td>
        </tr>
        <tr>
            <td><a href="https://huggingface.co/datasets/jondurbin/airoboros-gpt4-1.4.1">airoboros-gpt4-1.4.1</a></td>
            <td>other</td>
            <td>13</td>
        </tr>
        <tr>
            <td><a href="https://huggingface.co/datasets/TigerResearch/tigerbot-kaggle-leetcodesolutions-en-2k/viewer/TigerResearch--tigerbot-kaggle-leetcodesolutions-en-2k/train?p=2">tigerbot-kaggle-leetcodesolutions-en-2k</a></td>
            <td>apache-2.0</td>
            <td>0</td>
        </tr>
        <tr>
            <td><a href="https://huggingface.co/datasets/openbookqa/viewer/additional/train?row=35">OpenBookQA: A New Dataset for Open Book Question Answering</a></td>
            <td>apache-2.0</td>
            <td>6</td>
        </tr>
        <tr>
            <td><a href="https://arb.duckai.org">ARB: Advanced Reasoning Benchmark for Large Language Models</a></td>
            <td>MIT</td>
            <td>0</td>
        </tr>
        <tr>
            <td><a href="https://huggingface.co/datasets/timdettmers/openassistant-guanaco">Openassistant-guanaco</a></td>
            <td>apache-2.0</td>
            <td>13</td>
        </tr>
        <tr>
            <td>*<a href="https://huggingface.co/datasets/ehartford/dolphin">ehartford/dolphin</a> (first 25k rows)</td>
            <td>apache-2.0</td>
            <td>0</td>
        </tr>
    </tbody>
</table>

</div>

<div class="content">
  <h2>Contamination</h2>
  <p>Our methodology prioritizes preventing benchmark test questions from leaking into the training set to avoid biasing results through mere memorization. While we strive for accuracy, we recognize the need for flexibility in marking questions as duplicates, given the varied ways a question can be posed and the influence of general domain knowledge. To manage potential leaks, we crafted heuristics for manually filtering questions from Open-Platypus with over 80% cosine embedding similarity to benchmark questions. We classify potential leaks into three categories: (1) duplicate (2) gray-area and (3) similar but different. To err on the side of caution, we exclude all groups from our training set.</p>

<h3>Duplicate</h3>
<p>These are near-exact replicas of test set questions, with perhaps a minor word change or slight rearrangement. This is the only category we count as "true" contamination, as defined by the number of leaked questions in the table above. Specific examples of this can be seen in:</p>

<figure style="text-align: center;">
    <img src="./Platypus_files/dups.png" style="width: 60%; height: auto; display: block; margin: 0 auto;">
    <figcaption>Figure 1: Examples of true contamination.</figcaption>
</figure>

<h3>Gray-area</h3>
<p>The next group, termed gray-area, encompasses questions that are not exact duplicates and fall within the realm of general knowledge. While we leave the final judgement of these questions to the open-source community, we believe they often necessitate expert knowledge. Notably, this category includes questions with identical instructions but answers that are synonymous:</p>

<figure style="text-align: center;">
    <img src="./Platypus_files/gray.png" style="width: 60%; height: auto; display: block; margin: 0 auto;">
    <figcaption>Figure 2: Examples of gray-area questions.</figcaption>
</figure>

<h3>Similar but different</h3>
<p>These questions have high similarity scores but differ significantly in answers due to subtle question variations. Please see the paper for detailed examples.</p>

</div>

<div class="content">
  <h2>Fine-tuning & Merging</h2> <p>
After refining our dataset, we center on two methods: Low Rank Approximation (LoRA) training and Parameter-Efficient Fine-Tuning (PEFT) library. Unlike full fine-tuning, LoRA preserves pre-trained model weights, integrating rank decomposition matrices in transformer layers. This cuts down trainable parameters, saving on training time and cost.

Initially, our fine-tuning honed in on attention modules like <code>v_proj</code>, <code>q_proj</code>, <code>k_proj</code>, and <code>o_proj</code>. Later, based on insights from He et al., we transitioned to <code>gate_proj</code>, <code>down_proj</code>, and <code>up_proj</code> modules. These showed better results except when trainable parameters were less than 0.1% of the total. We applied this uniformly to both our 13B and 70B models, which resulted in 0.27% and 0.2% trainable parameters. The only variance was the initial learning rate between these models. For an in depth breakdown of our pipeline, pleaase refer to the paper. </p>
</div>

<div class="content">
  <h2>Results</h2>
  <p></p>Hugging Face Open LLM Leaderboard as of 8/10/23. <br><br>
  <img src="./Platypus_files/HF.png" style="width: 100%; height: auto; display: block; margin: 0 auto;">
</div>

<div class="content">
  <h2>Limitations</h2>
  <p>Platypus, as a fine-tuned extension of LLaMa-2, retains many of the foundational model's constraints and introduces specific challenges due to its targeted training. It shares LLaMa-2's static knowledge base, which may become outdated. There's also a risk of generating inaccurate or inappropriate content, especially with unclear prompts. Although Platypus is enhanced for STEM and logic in English, its proficiency in other languages is not assured and can be inconsistent. It may occasionally produce content that's biased, offensive, or harmful. Efforts to mitigate these issues have been made, but challenges, especially in non-English languages, remain.<br><br>

The potential misuse of Platypus for malicious activities is a concern. Developers should conduct safety testing tailored to their application before deployment. Platypus may have limitations outside its primary domain, so users should exercise caution and consider additional fine-tuning for optimal performance. Users should ensure no overlap between Platypus's training data and other benchmark test sets. We've been cautious about data contamination and have avoided merging with models trained on tainted datasets. While we are confident there is no contamination in our cleaned training data, it is unlikely but not impossible that some questions slipped through the cracks. For a comprehensive understanding of these limitations, please refer to the limitations section in the paper.</p>
  <br>
</div>

<div class="content">
  <h2>BibTeX</h2>
  <code> @article{platypus2023,<br>
  &nbsp;&nbsp;title={Platypus: Quick, Cheap, and Powerful Refinement of LLMs},<br>
  &nbsp;&nbsp;author={Ariel N. Lee and Cole J. Hunter and Nataniel Ruiz},<br>
  &nbsp;&nbsp;booktitle={arXiv preprint arxiv:2308.07317},<br>
  &nbsp;&nbsp;year={2023}<br>
  } </code> 
</div>

<div class="content" id="acknowledgements">
  <p><strong>Acknowledgements</strong>:
    A very special thank you to both Hugging Face, for creating a space where anyone can evaluate and release LLMs, and Meta AI for sharing LLaMa-2, the backbone of our fine-tuned models. We would also like to thank the creators of LoRA, without whom we could not have afforded to fine-tune a 70B variant of LLaMa-2. This was a fun learning experience made possible through the open-source community.
    <!-- USED DREAMBOOTH AS TEMPLATE --> 
  </p>
</div>
</body>
</html>
